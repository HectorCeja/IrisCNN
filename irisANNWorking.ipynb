{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     a    b    c    d        class\n",
       "0  5.1  3.5  1.4  0.2  Iris-setosa\n",
       "1  4.9  3.0  1.4  0.2  Iris-setosa\n",
       "2  4.7  3.2  1.3  0.2  Iris-setosa\n",
       "3  4.6  3.1  1.5  0.2  Iris-setosa\n",
       "4  5.0  3.6  1.4  0.2  Iris-setosa\n",
       "5  5.4  3.9  1.7  0.4  Iris-setosa"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "dataset = pd.read_csv('iris.csv')\n",
    "dataset.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelEncoder()\n",
    "lb.fit(dataset['class'])\n",
    "last_column_numeric = lb.transform(dataset['class'])\n",
    "last_column_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     a    b    c    d  group\n",
       "0  5.1  3.5  1.4  0.2      0\n",
       "1  4.9  3.0  1.4  0.2      0\n",
       "2  4.7  3.2  1.3  0.2      0\n",
       "3  4.6  3.1  1.5  0.2      0\n",
       "4  5.0  3.6  1.4  0.2      0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.drop(columns='class', inplace=True)\n",
    "dataset['group'] = last_column_numeric\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "dataset_soft = dataset.rolling(window=8, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-5)</th>\n",
       "      <th>var2(t-5)</th>\n",
       "      <th>var3(t-5)</th>\n",
       "      <th>var4(t-5)</th>\n",
       "      <th>var5(t-5)</th>\n",
       "      <th>var1(t-4)</th>\n",
       "      <th>var2(t-4)</th>\n",
       "      <th>var3(t-4)</th>\n",
       "      <th>var4(t-4)</th>\n",
       "      <th>var5(t-4)</th>\n",
       "      <th>...</th>\n",
       "      <th>var1(t-1)</th>\n",
       "      <th>var2(t-1)</th>\n",
       "      <th>var3(t-1)</th>\n",
       "      <th>var4(t-1)</th>\n",
       "      <th>var5(t-1)</th>\n",
       "      <th>var1(t)</th>\n",
       "      <th>var2(t)</th>\n",
       "      <th>var3(t)</th>\n",
       "      <th>var4(t)</th>\n",
       "      <th>var5(t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.1000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.8600</td>\n",
       "      <td>3.280000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.9500</td>\n",
       "      <td>3.383333</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.0000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.9000</td>\n",
       "      <td>3.233333</td>\n",
       "      <td>1.366667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.9500</td>\n",
       "      <td>3.383333</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.9000</td>\n",
       "      <td>3.385714</td>\n",
       "      <td>1.442857</td>\n",
       "      <td>0.242857</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.9000</td>\n",
       "      <td>3.233333</td>\n",
       "      <td>1.366667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.8250</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.9000</td>\n",
       "      <td>3.385714</td>\n",
       "      <td>1.442857</td>\n",
       "      <td>0.242857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.9125</td>\n",
       "      <td>3.387500</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.8250</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.8600</td>\n",
       "      <td>3.280000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.9125</td>\n",
       "      <td>3.387500</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.8250</td>\n",
       "      <td>3.312500</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.8600</td>\n",
       "      <td>3.280000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.9500</td>\n",
       "      <td>3.383333</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.8250</td>\n",
       "      <td>3.312500</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.8250</td>\n",
       "      <td>3.325000</td>\n",
       "      <td>1.462500</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.9500</td>\n",
       "      <td>3.383333</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.9000</td>\n",
       "      <td>3.385714</td>\n",
       "      <td>1.442857</td>\n",
       "      <td>0.242857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.8250</td>\n",
       "      <td>3.325000</td>\n",
       "      <td>1.462500</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.9125</td>\n",
       "      <td>3.387500</td>\n",
       "      <td>1.487500</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.9000</td>\n",
       "      <td>3.385714</td>\n",
       "      <td>1.442857</td>\n",
       "      <td>0.242857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.9125</td>\n",
       "      <td>3.387500</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.9125</td>\n",
       "      <td>3.387500</td>\n",
       "      <td>1.487500</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.9375</td>\n",
       "      <td>3.425000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.9125</td>\n",
       "      <td>3.387500</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.8250</td>\n",
       "      <td>3.312500</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.9375</td>\n",
       "      <td>3.425000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.9125</td>\n",
       "      <td>3.350000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.8250</td>\n",
       "      <td>3.312500</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.8250</td>\n",
       "      <td>3.325000</td>\n",
       "      <td>1.462500</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.9125</td>\n",
       "      <td>3.350000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.7750</td>\n",
       "      <td>3.237500</td>\n",
       "      <td>1.425000</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.8250</td>\n",
       "      <td>3.325000</td>\n",
       "      <td>1.462500</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.9125</td>\n",
       "      <td>3.387500</td>\n",
       "      <td>1.487500</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.7750</td>\n",
       "      <td>3.237500</td>\n",
       "      <td>1.425000</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.9250</td>\n",
       "      <td>3.312500</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    var1(t-5)  var2(t-5)  var3(t-5)  var4(t-5)  var5(t-5)  var1(t-4)  \\\n",
       "5      5.1000   3.500000   1.400000   0.200000        0.0     5.0000   \n",
       "6      5.0000   3.250000   1.400000   0.200000        0.0     4.9000   \n",
       "7      4.9000   3.233333   1.366667   0.200000        0.0     4.8250   \n",
       "8      4.8250   3.200000   1.400000   0.200000        0.0     4.8600   \n",
       "9      4.8600   3.280000   1.400000   0.200000        0.0     4.9500   \n",
       "10     4.9500   3.383333   1.450000   0.233333        0.0     4.9000   \n",
       "11     4.9000   3.385714   1.442857   0.242857        0.0     4.9125   \n",
       "12     4.9125   3.387500   1.450000   0.237500        0.0     4.8250   \n",
       "13     4.8250   3.312500   1.450000   0.237500        0.0     4.8250   \n",
       "14     4.8250   3.325000   1.462500   0.225000        0.0     4.9125   \n",
       "\n",
       "    var2(t-4)  var3(t-4)  var4(t-4)  var5(t-4)   ...     var1(t-1)  var2(t-1)  \\\n",
       "5    3.250000   1.400000   0.200000        0.0   ...        4.8600   3.280000   \n",
       "6    3.233333   1.366667   0.200000        0.0   ...        4.9500   3.383333   \n",
       "7    3.200000   1.400000   0.200000        0.0   ...        4.9000   3.385714   \n",
       "8    3.280000   1.400000   0.200000        0.0   ...        4.9125   3.387500   \n",
       "9    3.383333   1.450000   0.233333        0.0   ...        4.8250   3.312500   \n",
       "10   3.385714   1.442857   0.242857        0.0   ...        4.8250   3.325000   \n",
       "11   3.387500   1.450000   0.237500        0.0   ...        4.9125   3.387500   \n",
       "12   3.312500   1.450000   0.237500        0.0   ...        4.9375   3.425000   \n",
       "13   3.325000   1.462500   0.225000        0.0   ...        4.9125   3.350000   \n",
       "14   3.387500   1.487500   0.225000        0.0   ...        4.7750   3.237500   \n",
       "\n",
       "    var3(t-1)  var4(t-1)  var5(t-1)  var1(t)   var2(t)   var3(t)   var4(t)  \\\n",
       "5    1.400000   0.200000        0.0   4.9500  3.383333  1.450000  0.233333   \n",
       "6    1.450000   0.233333        0.0   4.9000  3.385714  1.442857  0.242857   \n",
       "7    1.442857   0.242857        0.0   4.9125  3.387500  1.450000  0.237500   \n",
       "8    1.450000   0.237500        0.0   4.8250  3.312500  1.450000  0.237500   \n",
       "9    1.450000   0.237500        0.0   4.8250  3.325000  1.462500  0.225000   \n",
       "10   1.462500   0.225000        0.0   4.9125  3.387500  1.487500  0.225000   \n",
       "11   1.487500   0.225000        0.0   4.9375  3.425000  1.500000  0.225000   \n",
       "12   1.500000   0.225000        0.0   4.9125  3.350000  1.500000  0.212500   \n",
       "13   1.500000   0.212500        0.0   4.7750  3.237500  1.425000  0.175000   \n",
       "14   1.425000   0.175000        0.0   4.9250  3.312500  1.400000  0.162500   \n",
       "\n",
       "    var5(t)  \n",
       "5       0.0  \n",
       "6       0.0  \n",
       "7       0.0  \n",
       "8       0.0  \n",
       "9       0.0  \n",
       "10      0.0  \n",
       "11      0.0  \n",
       "12      0.0  \n",
       "13      0.0  \n",
       "14      0.0  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Tools as tls\n",
    "\n",
    "size = 5\n",
    "\n",
    "dataset_serialized = tls.series_to_supervised(dataset_soft,size, n_out=1, dropnan = True)\n",
    "dataset_serialized.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2         3    4         5         6   \\\n",
      "0    0.151163  0.736842  0.013889  0.018987  0.0  0.104651  0.526316   \n",
      "1    0.104651  0.526316  0.013889  0.018987  0.0  0.058140  0.512281   \n",
      "2    0.058140  0.512281  0.006481  0.018987  0.0  0.023256  0.484211   \n",
      "3    0.023256  0.484211  0.013889  0.018987  0.0  0.039535  0.551579   \n",
      "4    0.039535  0.551579  0.013889  0.018987  0.0  0.081395  0.638596   \n",
      "5    0.081395  0.638596  0.025000  0.035865  0.0  0.058140  0.640602   \n",
      "6    0.058140  0.640602  0.023413  0.040687  0.0  0.063953  0.642105   \n",
      "7    0.063953  0.642105  0.025000  0.037975  0.0  0.023256  0.578947   \n",
      "8    0.023256  0.578947  0.025000  0.037975  0.0  0.023256  0.589474   \n",
      "9    0.023256  0.589474  0.027778  0.031646  0.0  0.063953  0.642105   \n",
      "10   0.063953  0.642105  0.033333  0.031646  0.0  0.075581  0.673684   \n",
      "11   0.075581  0.673684  0.036111  0.031646  0.0  0.063953  0.610526   \n",
      "12   0.063953  0.610526  0.036111  0.025316  0.0  0.000000  0.515789   \n",
      "13   0.000000  0.515789  0.019444  0.006329  0.0  0.069767  0.578947   \n",
      "14   0.069767  0.578947  0.013889  0.000000  0.0  0.110465  0.684211   \n",
      "15   0.110465  0.684211  0.013889  0.012658  0.0  0.168605  0.789474   \n",
      "16   0.168605  0.789474  0.011111  0.025316  0.0  0.180233  0.831579   \n",
      "17   0.180233  0.831579  0.008333  0.037975  0.0  0.197674  0.842105   \n",
      "18   0.197674  0.842105  0.013889  0.044304  0.0  0.215116  0.884211   \n",
      "19   0.215116  0.884211  0.011111  0.050633  0.0  0.250000  0.926316   \n",
      "20   0.250000  0.926316  0.019444  0.056962  0.0  0.296512  1.000000   \n",
      "21   0.296512  1.000000  0.030556  0.075949  0.0  0.226744  0.957895   \n",
      "22   0.226744  0.957895  0.025000  0.075949  0.0  0.191860  0.842105   \n",
      "23   0.191860  0.842105  0.030556  0.082278  0.0  0.156977  0.789474   \n",
      "24   0.156977  0.789474  0.047222  0.069620  0.0  0.151163  0.736842   \n",
      "25   0.151163  0.736842  0.052778  0.063291  0.0  0.110465  0.694737   \n",
      "26   0.110465  0.694737  0.050000  0.069620  0.0  0.116279  0.663158   \n",
      "27   0.116279  0.663158  0.050000  0.063291  0.0  0.104651  0.663158   \n",
      "28   0.104651  0.663158  0.041667  0.063291  0.0  0.081395  0.610526   \n",
      "29   0.081395  0.610526  0.044444  0.050633  0.0  0.093023  0.557895   \n",
      "..        ...       ...       ...       ...  ...       ...       ...   \n",
      "115  0.773256  0.263158  0.902778  0.993671  1.0  0.761628  0.315789   \n",
      "116  0.761628  0.315789  0.894444  0.993671  1.0  0.790698  0.336842   \n",
      "117  0.790698  0.336842  0.911111  0.974684  1.0  0.860465  0.273684   \n",
      "118  0.860465  0.273684  0.961111  0.993671  1.0  0.837209  0.221053   \n",
      "119  0.837209  0.221053  0.952778  0.968354  1.0  0.843023  0.242105   \n",
      "120  0.843023  0.242105  0.958333  0.981013  1.0  0.837209  0.273684   \n",
      "121  0.837209  0.273684  0.955556  0.981013  1.0  0.947674  0.273684   \n",
      "122  0.947674  0.273684  1.000000  0.955696  1.0  0.941860  0.221053   \n",
      "123  0.941860  0.221053  0.988889  0.924051  1.0  0.953488  0.252632   \n",
      "124  0.953488  0.252632  0.994444  0.943038  1.0  0.924419  0.189474   \n",
      "125  0.924419  0.189474  0.975000  0.917722  1.0  0.837209  0.210526   \n",
      "126  0.837209  0.210526  0.916667  0.886076  1.0  0.843023  0.294737   \n",
      "127  0.843023  0.294737  0.913889  0.905063  1.0  0.813953  0.252632   \n",
      "128  0.813953  0.252632  0.911111  0.892405  1.0  0.906977  0.273684   \n",
      "129  0.906977  0.273684  0.936111  0.867089  1.0  0.889535  0.273684   \n",
      "130  0.889535  0.273684  0.919444  0.860759  1.0  0.982558  0.389474   \n",
      "131  0.982558  0.389474  0.961111  0.873418  1.0  0.965116  0.336842   \n",
      "132  0.965116  0.336842  0.958333  0.879747  1.0  0.912791  0.294737   \n",
      "133  0.912791  0.294737  0.933333  0.860759  1.0  0.906977  0.273684   \n",
      "134  0.906977  0.273684  0.955556  0.835443  1.0  1.000000  0.273684   \n",
      "135  1.000000  0.273684  0.988889  0.867089  1.0  0.994186  0.336842   \n",
      "136  0.994186  0.336842  0.988889  0.886076  1.0  0.947674  0.347368   \n",
      "137  0.947674  0.347368  0.980556  0.898734  1.0  0.866279  0.368421   \n",
      "138  0.866279  0.368421  0.944444  0.892405  1.0  0.808140  0.294737   \n",
      "139  0.808140  0.294737  0.916667  0.898734  1.0  0.825581  0.326316   \n",
      "140  0.825581  0.326316  0.916667  0.911392  1.0  0.860465  0.357895   \n",
      "141  0.860465  0.357895  0.916667  0.962025  1.0  0.843023  0.368421   \n",
      "142  0.843023  0.368421  0.902778  0.993671  1.0  0.790698  0.389474   \n",
      "143  0.790698  0.389474  0.897222  0.993671  1.0  0.813953  0.378947   \n",
      "144  0.813953  0.378947  0.900000  1.000000  1.0  0.831395  0.368421   \n",
      "\n",
      "           7         8    9  ...         20        21        22        23  \\\n",
      "0    0.013889  0.018405  0.0 ...   0.039535  0.551579  0.013889  0.018293   \n",
      "1    0.006481  0.018405  0.0 ...   0.081395  0.638596  0.025000  0.034553   \n",
      "2    0.013889  0.018405  0.0 ...   0.058140  0.640602  0.023413  0.039199   \n",
      "3    0.013889  0.018405  0.0 ...   0.063953  0.642105  0.025000  0.036585   \n",
      "4    0.025000  0.034765  0.0 ...   0.023256  0.578947  0.025000  0.036585   \n",
      "5    0.023413  0.039439  0.0 ...   0.023256  0.589474  0.027778  0.030488   \n",
      "6    0.025000  0.036810  0.0 ...   0.063953  0.642105  0.033333  0.030488   \n",
      "7    0.025000  0.036810  0.0 ...   0.075581  0.673684  0.036111  0.030488   \n",
      "8    0.027778  0.030675  0.0 ...   0.063953  0.610526  0.036111  0.024390   \n",
      "9    0.033333  0.030675  0.0 ...   0.000000  0.515789  0.019444  0.006098   \n",
      "10   0.036111  0.030675  0.0 ...   0.069767  0.578947  0.013889  0.000000   \n",
      "11   0.036111  0.024540  0.0 ...   0.110465  0.684211  0.013889  0.012195   \n",
      "12   0.019444  0.006135  0.0 ...   0.168605  0.789474  0.011111  0.024390   \n",
      "13   0.013889  0.000000  0.0 ...   0.180233  0.831579  0.008333  0.036585   \n",
      "14   0.013889  0.012270  0.0 ...   0.197674  0.842105  0.013889  0.042683   \n",
      "15   0.011111  0.024540  0.0 ...   0.215116  0.884211  0.011111  0.048780   \n",
      "16   0.008333  0.036810  0.0 ...   0.250000  0.926316  0.019444  0.054878   \n",
      "17   0.013889  0.042945  0.0 ...   0.296512  1.000000  0.030556  0.073171   \n",
      "18   0.011111  0.049080  0.0 ...   0.226744  0.957895  0.025000  0.073171   \n",
      "19   0.019444  0.055215  0.0 ...   0.191860  0.842105  0.030556  0.079268   \n",
      "20   0.030556  0.073620  0.0 ...   0.156977  0.789474  0.047222  0.067073   \n",
      "21   0.025000  0.073620  0.0 ...   0.151163  0.736842  0.052778  0.060976   \n",
      "22   0.030556  0.079755  0.0 ...   0.110465  0.694737  0.050000  0.067073   \n",
      "23   0.047222  0.067485  0.0 ...   0.116279  0.663158  0.050000  0.060976   \n",
      "24   0.052778  0.061350  0.0 ...   0.104651  0.663158  0.041667  0.060976   \n",
      "25   0.050000  0.067485  0.0 ...   0.081395  0.610526  0.044444  0.048780   \n",
      "26   0.050000  0.061350  0.0 ...   0.093023  0.557895  0.061111  0.048780   \n",
      "27   0.041667  0.061350  0.0 ...   0.110465  0.568421  0.055556  0.042683   \n",
      "28   0.044444  0.049080  0.0 ...   0.133721  0.642105  0.044444  0.036585   \n",
      "29   0.061111  0.049080  0.0 ...   0.162791  0.768421  0.038889  0.036585   \n",
      "..        ...       ...  ... ...        ...       ...       ...       ...   \n",
      "115  0.894444  0.963190  1.0 ...   0.837209  0.221053  0.952778  0.932927   \n",
      "116  0.911111  0.944785  1.0 ...   0.843023  0.242105  0.958333  0.945122   \n",
      "117  0.961111  0.963190  1.0 ...   0.837209  0.273684  0.955556  0.945122   \n",
      "118  0.952778  0.938650  1.0 ...   0.947674  0.273684  1.000000  0.920732   \n",
      "119  0.958333  0.950920  1.0 ...   0.941860  0.221053  0.988889  0.890244   \n",
      "120  0.955556  0.950920  1.0 ...   0.953488  0.252632  0.994444  0.908537   \n",
      "121  1.000000  0.926380  1.0 ...   0.924419  0.189474  0.975000  0.884146   \n",
      "122  0.988889  0.895706  1.0 ...   0.837209  0.210526  0.916667  0.853659   \n",
      "123  0.994444  0.914110  1.0 ...   0.843023  0.294737  0.913889  0.871951   \n",
      "124  0.975000  0.889571  1.0 ...   0.813953  0.252632  0.911111  0.859756   \n",
      "125  0.916667  0.858896  1.0 ...   0.906977  0.273684  0.936111  0.835366   \n",
      "126  0.913889  0.877301  1.0 ...   0.889535  0.273684  0.919444  0.829268   \n",
      "127  0.911111  0.865031  1.0 ...   0.982558  0.389474  0.961111  0.841463   \n",
      "128  0.936111  0.840491  1.0 ...   0.965116  0.336842  0.958333  0.847561   \n",
      "129  0.919444  0.834356  1.0 ...   0.912791  0.294737  0.933333  0.829268   \n",
      "130  0.961111  0.846626  1.0 ...   0.906977  0.273684  0.955556  0.804878   \n",
      "131  0.958333  0.852761  1.0 ...   1.000000  0.273684  0.988889  0.835366   \n",
      "132  0.933333  0.834356  1.0 ...   0.994186  0.336842  0.988889  0.853659   \n",
      "133  0.955556  0.809816  1.0 ...   0.947674  0.347368  0.980556  0.865854   \n",
      "134  0.988889  0.840491  1.0 ...   0.866279  0.368421  0.944444  0.859756   \n",
      "135  0.988889  0.858896  1.0 ...   0.808140  0.294737  0.916667  0.865854   \n",
      "136  0.980556  0.871166  1.0 ...   0.825581  0.326316  0.916667  0.878049   \n",
      "137  0.944444  0.865031  1.0 ...   0.860465  0.357895  0.916667  0.926829   \n",
      "138  0.916667  0.871166  1.0 ...   0.843023  0.368421  0.902778  0.957317   \n",
      "139  0.916667  0.883436  1.0 ...   0.790698  0.389474  0.897222  0.957317   \n",
      "140  0.916667  0.932515  1.0 ...   0.813953  0.378947  0.900000  0.963415   \n",
      "141  0.902778  0.963190  1.0 ...   0.831395  0.368421  0.891667  0.993902   \n",
      "142  0.897222  0.963190  1.0 ...   0.848837  0.315789  0.897222  1.000000   \n",
      "143  0.900000  0.969325  1.0 ...   0.825581  0.305263  0.891667  0.993902   \n",
      "144  0.891667  1.000000  1.0 ...   0.796512  0.336842  0.886111  0.987805   \n",
      "\n",
      "      24        25        26        27        28   29  \n",
      "0    0.0  0.081395  0.638596  0.025000  0.034553  0.0  \n",
      "1    0.0  0.058140  0.640602  0.023413  0.039199  0.0  \n",
      "2    0.0  0.063953  0.642105  0.025000  0.036585  0.0  \n",
      "3    0.0  0.023256  0.578947  0.025000  0.036585  0.0  \n",
      "4    0.0  0.023256  0.589474  0.027778  0.030488  0.0  \n",
      "5    0.0  0.063953  0.642105  0.033333  0.030488  0.0  \n",
      "6    0.0  0.075581  0.673684  0.036111  0.030488  0.0  \n",
      "7    0.0  0.063953  0.610526  0.036111  0.024390  0.0  \n",
      "8    0.0  0.000000  0.515789  0.019444  0.006098  0.0  \n",
      "9    0.0  0.069767  0.578947  0.013889  0.000000  0.0  \n",
      "10   0.0  0.110465  0.684211  0.013889  0.012195  0.0  \n",
      "11   0.0  0.168605  0.789474  0.011111  0.024390  0.0  \n",
      "12   0.0  0.180233  0.831579  0.008333  0.036585  0.0  \n",
      "13   0.0  0.197674  0.842105  0.013889  0.042683  0.0  \n",
      "14   0.0  0.215116  0.884211  0.011111  0.048780  0.0  \n",
      "15   0.0  0.250000  0.926316  0.019444  0.054878  0.0  \n",
      "16   0.0  0.296512  1.000000  0.030556  0.073171  0.0  \n",
      "17   0.0  0.226744  0.957895  0.025000  0.073171  0.0  \n",
      "18   0.0  0.191860  0.842105  0.030556  0.079268  0.0  \n",
      "19   0.0  0.156977  0.789474  0.047222  0.067073  0.0  \n",
      "20   0.0  0.151163  0.736842  0.052778  0.060976  0.0  \n",
      "21   0.0  0.110465  0.694737  0.050000  0.067073  0.0  \n",
      "22   0.0  0.116279  0.663158  0.050000  0.060976  0.0  \n",
      "23   0.0  0.104651  0.663158  0.041667  0.060976  0.0  \n",
      "24   0.0  0.081395  0.610526  0.044444  0.048780  0.0  \n",
      "25   0.0  0.093023  0.557895  0.061111  0.048780  0.0  \n",
      "26   0.0  0.110465  0.568421  0.055556  0.042683  0.0  \n",
      "27   0.0  0.133721  0.642105  0.044444  0.036585  0.0  \n",
      "28   0.0  0.162791  0.768421  0.038889  0.036585  0.0  \n",
      "29   0.0  0.156977  0.736842  0.036111  0.018293  0.0  \n",
      "..   ...       ...       ...       ...       ...  ...  \n",
      "115  1.0  0.843023  0.242105  0.958333  0.945122  1.0  \n",
      "116  1.0  0.837209  0.273684  0.955556  0.945122  1.0  \n",
      "117  1.0  0.947674  0.273684  1.000000  0.920732  1.0  \n",
      "118  1.0  0.941860  0.221053  0.988889  0.890244  1.0  \n",
      "119  1.0  0.953488  0.252632  0.994444  0.908537  1.0  \n",
      "120  1.0  0.924419  0.189474  0.975000  0.884146  1.0  \n",
      "121  1.0  0.837209  0.210526  0.916667  0.853659  1.0  \n",
      "122  1.0  0.843023  0.294737  0.913889  0.871951  1.0  \n",
      "123  1.0  0.813953  0.252632  0.911111  0.859756  1.0  \n",
      "124  1.0  0.906977  0.273684  0.936111  0.835366  1.0  \n",
      "125  1.0  0.889535  0.273684  0.919444  0.829268  1.0  \n",
      "126  1.0  0.982558  0.389474  0.961111  0.841463  1.0  \n",
      "127  1.0  0.965116  0.336842  0.958333  0.847561  1.0  \n",
      "128  1.0  0.912791  0.294737  0.933333  0.829268  1.0  \n",
      "129  1.0  0.906977  0.273684  0.955556  0.804878  1.0  \n",
      "130  1.0  1.000000  0.273684  0.988889  0.835366  1.0  \n",
      "131  1.0  0.994186  0.336842  0.988889  0.853659  1.0  \n",
      "132  1.0  0.947674  0.347368  0.980556  0.865854  1.0  \n",
      "133  1.0  0.866279  0.368421  0.944444  0.859756  1.0  \n",
      "134  1.0  0.808140  0.294737  0.916667  0.865854  1.0  \n",
      "135  1.0  0.825581  0.326316  0.916667  0.878049  1.0  \n",
      "136  1.0  0.860465  0.357895  0.916667  0.926829  1.0  \n",
      "137  1.0  0.843023  0.368421  0.902778  0.957317  1.0  \n",
      "138  1.0  0.790698  0.389474  0.897222  0.957317  1.0  \n",
      "139  1.0  0.813953  0.378947  0.900000  0.963415  1.0  \n",
      "140  1.0  0.831395  0.368421  0.891667  0.993902  1.0  \n",
      "141  1.0  0.848837  0.315789  0.897222  1.000000  1.0  \n",
      "142  1.0  0.825581  0.305263  0.891667  0.993902  1.0  \n",
      "143  1.0  0.796512  0.336842  0.886111  0.987805  1.0  \n",
      "144  1.0  0.738372  0.326316  0.886111  0.957317  1.0  \n",
      "\n",
      "[145 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "datasetnorm = preprocessing.minmax_scale(dataset_serialized, feature_range=(0, 1))\n",
    "datasetnorm = DataFrame(datasetnorm)\n",
    "print(datasetnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "zy = datasetnorm.iloc[:, -1:]\n",
    "zx = datasetnorm.iloc[:, :-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116, 29) (116, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(zx, dtype='float64')\n",
    "y = np.array(zy, dtype='float64')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=20)\n",
    "\n",
    "print(X_train.shape, y_train.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116, 1, 29) (116, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "y_train = y_train.reshape((y_train.shape[0], 1, y_train.shape[1]))\n",
    "y_test = y_test.reshape((y_test.shape[0], 1, y_test.shape[1]))\n",
    "\n",
    "print(X_train.shape, y_train.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(10, input_shape=(29,), activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax', name='output'))\n",
    "\n",
    "# Adam optimizer with learning rate of 0.001\n",
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are passing a target array of shape (116, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-0a2e538ca8a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#history = model.fit(X_train, y_train, epochs=50, batch_size=24, validation_data=(X_test, y_test), verbose=1, shuffle=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    807\u001b[0m                 \u001b[0;31m# using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m                 check_loss_and_target_compatibility(\n\u001b[0;32m--> 809\u001b[0;31m                     y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 raise ValueError(\n\u001b[1;32m    272\u001b[0m                     \u001b[0;34m'You are passing a target array of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                     \u001b[0;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m                     \u001b[0;34m'`categorical_crossentropy` expects '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                     \u001b[0;34m'targets to be binary matrices (1s and 0s) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are passing a target array of shape (116, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
     ]
    }
   ],
   "source": [
    "#history = model.fit(X_train, y_train, epochs=50, batch_size=24, validation_data=(X_test, y_test), verbose=1, shuffle=False)\n",
    "history = model.fit(X_train, y_train, verbose=2, batch_size=4, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
